from fastapi import FastAPI, Request, File, UploadFile
from pydantic import BaseModel
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from dotenv import load_dotenv
import os
import uvicorn
import torch
from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
from uuid import uuid4
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

load_dotenv()

app = FastAPI()

# ===============================
# RATE LIMITING SETUP 
# ===============================
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# ===============================
# GLOBAL STATE (Multi-document support from remote)
# ===============================
VECTOR_STORE = None
DOCUMENT_REGISTRY = {}
DOCUMENT_EMBEDDINGS = {}

HF_GENERATION_MODEL = os.getenv("HF_GENERATION_MODEL", "google/flan-t5-small")

generation_tokenizer = None
generation_model = None
generation_is_encoder_decoder = False

# ===============================
# EMBEDDING MODEL
# ===============================
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# ===============================
# MODEL LOADING
# ===============================
def load_generation_model():
    global generation_tokenizer, generation_model, generation_is_encoder_decoder

    if generation_model is not None:
        return generation_tokenizer, generation_model, generation_is_encoder_decoder

    config = AutoConfig.from_pretrained(HF_GENERATION_MODEL)
    generation_is_encoder_decoder = bool(getattr(config, "is_encoder_decoder", False))
    generation_tokenizer = AutoTokenizer.from_pretrained(HF_GENERATION_MODEL)

    if generation_is_encoder_decoder:
        generation_model = AutoModelForSeq2SeqLM.from_pretrained(HF_GENERATION_MODEL)
    else:
        generation_model = AutoModelForCausalLM.from_pretrained(HF_GENERATION_MODEL)

    if torch.cuda.is_available():
        generation_model = generation_model.to("cuda")

    generation_model.eval()
    return generation_tokenizer, generation_model, generation_is_encoder_decoder


def generate_response(prompt: str, max_new_tokens: int) -> str:
    tokenizer, model, is_encoder_decoder = load_generation_model()
    device = next(model.parameters()).device

    encoded = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    encoded = {k: v.to(device) for k, v in encoded.items()}
    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id

    with torch.no_grad():
        output_ids = model.generate(
            **encoded,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=pad_token_id,
        )

    if is_encoder_decoder:
        return tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

    input_len = encoded["input_ids"].shape[1]
    new_tokens = output_ids[0][input_len:]
    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()


# ===============================
# REQUEST MODELS
# ===============================
class PDFPath(BaseModel):
    filePath: str


class Question(BaseModel):
    question: str
    doc_ids: list[str] | None = None


class SummarizeRequest(BaseModel):
    doc_ids: list[str] | None = None


class CompareRequest(BaseModel):
    doc_ids: list[str]


# ===============================
# UPLOAD ENDPOINT (Expected by frontend)
# ===============================
@app.post("/upload") 
@limiter.limit("10/15 minutes")
async def upload_file(request: Request, file: UploadFile = File(...)):
    """Upload and process PDF file"""
    if not file.filename.lower().endswith('.pdf'):
        return {"error": "Only PDF files are supported"}
    
    # Save uploaded file temporarily
    upload_dir = "uploads"
    os.makedirs(upload_dir, exist_ok=True)
    file_path = os.path.join(upload_dir, f"{uuid4().hex}_{file.filename}")
    
    try:
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # Process the PDF
        result = process_pdf_internal(file_path)
        
        # Clean up temporary file (optional - you may want to keep it)
        # os.remove(file_path)
        
        return result
        
    except Exception as e:
        return {"error": f"Upload failed: {str(e)}"}


def process_pdf_internal(file_path: str):
    """Internal function to process PDF without rate limiting"""
    global VECTOR_STORE, DOCUMENT_REGISTRY, DOCUMENT_EMBEDDINGS

    if not os.path.exists(file_path):
        return {"error": "File not found."}

    loader = PyPDFLoader(file_path)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    chunks = splitter.split_documents(docs)

    if not chunks:
        return {"error": "No text chunks generated from the PDF."}

    doc_id = str(uuid4())
    filename = os.path.basename(file_path)

    for chunk in chunks:
        chunk.metadata = {
            "doc_id": doc_id,
            "filename": filename
        }

    if VECTOR_STORE is None:
        VECTOR_STORE = FAISS.from_documents(chunks, embedding_model)
    else:
        VECTOR_STORE.add_documents(chunks)

    embeddings = embedding_model.embed_documents(
        [c.page_content for c in chunks]
    )
    doc_vector = np.mean(embeddings, axis=0)
    DOCUMENT_EMBEDDINGS[doc_id] = doc_vector

    DOCUMENT_REGISTRY[doc_id] = {
        "filename": filename,
        "num_chunks": len(chunks)
    }

    return {
        "message": "PDF processed successfully",
        "doc_id": doc_id
    }


# ===============================
# PROCESS PDF (MULTI-DOC SUPPORT + RATE LIMITING)
# ===============================
@app.post("/process-pdf")
@limiter.limit("15/15 minutes")
def process_pdf(request: Request, data: PDFPath):
    """Process PDF from file path"""
    return process_pdf_internal(data.filePath)


# ===============================
# LIST DOCUMENTS
# ===============================
@app.get("/documents")
def list_documents():
    return DOCUMENT_REGISTRY


# ===============================
# SIMILARITY MATRIX
# ===============================
@app.get("/similarity-matrix")
def similarity_matrix():
    if len(DOCUMENT_EMBEDDINGS) < 2:
        return {"error": "At least 2 documents required."}

    doc_ids = list(DOCUMENT_EMBEDDINGS.keys())
    vectors = np.array([DOCUMENT_EMBEDDINGS[d] for d in doc_ids])
    sim_matrix = cosine_similarity(vectors)

    result = {}
    for i, doc_id in enumerate(doc_ids):
        result[doc_id] = {}
        for j, other_id in enumerate(doc_ids):
            result[doc_id][other_id] = float(sim_matrix[i][j])

    return result


# ===============================
# ASK QUESTION (MULTI-DOC FILTER + RATE LIMITING)
# ===============================
@app.post("/ask")
@limiter.limit("60/15 minutes")
def ask_question(request: Request, data: Question):
    global VECTOR_STORE

    if VECTOR_STORE is None:
        return {"answer": "Please upload at least one PDF first!"}

    docs = VECTOR_STORE.similarity_search(data.question, k=10)

    if data.doc_ids:
        docs = [d for d in docs if d.metadata.get("doc_id") in data.doc_ids]

    if not docs:
        return {"answer": "No relevant context found."}

    context = "\n\n".join([d.page_content for d in docs])

    if data.doc_ids and len(data.doc_ids) > 1:
        prompt = (
            "You are an AI assistant comparing multiple documents.\n"
            "Clearly structure your answer as:\n"
            "- Similarities\n"
            "- Differences\n"
            "- Unique points per document\n\n"
            f"Context:\n{context}\n\n"
            f"Question: {data.question}\n"
            "Answer:"
        )
    else:
        prompt = (
            "You are a helpful assistant answering questions about a PDF.\n"
            "Use ONLY the provided context.\n\n"
            f"Context:\n{context}\n\n"
            f"Question: {data.question}\n"
            "Answer:"
        )

    answer = generate_response(prompt, max_new_tokens=300)
    return {"answer": answer}


# ===============================
# SUMMARIZE (MULTI-DOC + RATE LIMITING)
# ===============================
@app.post("/summarize")
@limiter.limit("15/15 minutes")
def summarize_pdf(request: Request, data: SummarizeRequest):
    global VECTOR_STORE

    if VECTOR_STORE is None:
        return {"summary": "Please upload at least one PDF first!"}

    docs = VECTOR_STORE.similarity_search("Summarize the document.", k=12)

    if data.doc_ids:
        docs = [d for d in docs if d.metadata.get("doc_id") in data.doc_ids]

    if not docs:
        return {"summary": "No document context available."}

    context = "\n\n".join([d.page_content for d in docs])

    prompt = (
        "Summarize the content in 6-8 concise bullet points.\n\n"
        f"Context:\n{context}\n\n"
        "Summary:"
    )

    summary = generate_response(prompt, max_new_tokens=250)
    return {"summary": summary}


# ===============================
# NEW: COMPARE SELECTED DOCUMENTS
# ===============================
@app.post("/compare")
def compare_documents(data: CompareRequest):
    global VECTOR_STORE, DOCUMENT_REGISTRY

    if VECTOR_STORE is None:
        return {"comparison": "Upload documents first."}

    if len(data.doc_ids) < 2:
        return {"comparison": "Select at least 2 documents."}

    # Pull more candidates
    docs = VECTOR_STORE.similarity_search("Main topics and differences.", k=15)

    # Filter safely
    docs = [d for d in docs if d.metadata.get("doc_id") in data.doc_ids]

    if not docs:
        return {"comparison": "No comparable content found."}

    # Limit per document to avoid overload
    grouped = {}
    for d in docs:
        grouped.setdefault(d.metadata["doc_id"], []).append(d.page_content)

    context = ""
    for doc_id in data.doc_ids:
        filename = DOCUMENT_REGISTRY.get(doc_id, {}).get("filename", doc_id)
        content = "\n\n".join(grouped.get(doc_id, [])[:4])
        context += f"\n\nDocument: {filename}\n{content}\n"

    prompt = (
        "You are an expert AI that compares documents.\n"
        "Provide a detailed comparison with:\n"
        "1. Overall Themes\n"
        "2. Key Similarities\n"
        "3. Key Differences\n"
        "4. Unique Strengths per Document\n\n"
        f"{context}\n\n"
        "Comparison:"
    )

    result = generate_response(prompt, max_new_tokens=600)

    return {"comparison": result}


if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=5000)